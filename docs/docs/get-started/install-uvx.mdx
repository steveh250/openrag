---
title: Invoke OpenRAG with uvx
slug: /install-uvx
---

For guided configuration and simplified service management, install OpenRAG with services managed by the [Terminal User Interface (TUI)](/tui).

You can use [`uvx`](https://docs.astral.sh/uv/guides/tools/#running-tools) to invoke OpenRAG outside of a Python project or without modifying your project's dependencies.

:::tip
The [automatic installer script](/install) also uses `uvx` to install OpenRAG.
:::

This installation method is best for testing OpenRAG by running it outside of a Python project.
For other installation methods, see [Choose an installation method](/install-options).

## Prerequisites

- For Microsoft Windows, you must use the Windows Subsystem for Linux (WSL).
See [Install OpenRAG on Windows](/install-windows) before proceeding.

- Install [Python](https://www.python.org/downloads/release/python-3100/) version 3.13 or later.

- Install [uv](https://docs.astral.sh/uv/getting-started/installation/).

- Install [Podman](https://podman.io/docs/installation) (recommended) or [Docker](https://docs.docker.com/get-docker/).

- Install [Podman Compose](https://docs.podman.io/en/latest/markdown/podman-compose.1.html) or [Docker Compose](https://docs.docker.com/compose/install/).
To use Docker Compose with Podman, you must alias Docker Compose commands to Podman commands.

- Gather the credentials and connection details for your preferred model providers.

   - OpenAI: Create an [OpenAI API key](https://platform.openai.com/api-keys).
   - Anthropic language models: Create an [Anthropic API key](https://www.anthropic.com/docs/api/reference).
   - IBM watsonx.ai: Get your watsonx.ai API endpoint, IBM project ID, and IBM API key from your watsonx deployment.
   - Ollama: Use the [Ollama documentation](https://docs.ollama.com/) to set up your Ollama instance locally, in the cloud, or on a remote server, and then get your Ollama server's base URL.

   You must have access to at least one language model and one embedding model.
   If your chosen provider offers both types, you can use the same provider for both models.
   If your provider offers only one type, such as Anthropic, you must select two providers.

- Optional: Install GPU support with an NVIDIA GPU, [CUDA](https://docs.nvidia.com/cuda/) support, and compatible NVIDIA drivers on the OpenRAG host machine. If you don't have GPU capabilities, OpenRAG provides an alternate CPU-only deployment.

## Install and run OpenRAG with uvx

1. Create a directory to store your OpenRAG configuration files and data, and then change to that directory:

   ```bash
   mkdir openrag-workspace
   cd openrag-workspace
   ```

2. Optional: If you want to use a pre-populated [`.env`](/reference/configuration) file for OpenRAG, copy it to this directory before invoking OpenRAG.

3. Invoke OpenRAG:

   ```bash
   uvx openrag
   ```

   You can invoke a specific version using any of the [`uvx` version specifiers](https://docs.astral.sh/uv/guides/tools/#requesting-specific-versions), such as `--from`:

   ```bash
   uvx --from openrag==0.1.30 openrag
   ```

   Invoking OpenRAG with `uvx openrag` creates a cached, ephemeral environment for the TUI in your local `uv` cache.
   By invoking OpenRAG in a specific directory, your OpenRAG configuration files and data are stored separately from the `uv` cache.
   Clearing the `uv` cache doesn't remove your entire OpenRAG installation.
   After clearing the cache, you can re-invoke OpenRAG (`uvx openrag`) to restart the TUI with your preserved configuration and data.

If you encounter errors during installation, see [Troubleshoot OpenRAG](/support/troubleshoot).

## Set up OpenRAG with the TUI

When the Terminal User Interface (TUI) starts, you must complete the initial setup to configure OpenRAG.

![OpenRAG TUI Interface](@site/static/img/OpenRAG_TUI_2025-09-10T13_04_11_757637.svg)

The OpenRAG setup process creates the `.env` and `docker-compose.yml` files in the directory where you invoked OpenRAG.
If it detects a `.env` file in the OpenRAG installation directory, it sources any variables from that file.

## Next steps

* Try some of OpenRAG's core features in the [quickstart](/quickstart#chat-with-documents).
* Learn how to [manage OpenRAG services](/manage-services).
* [Upload documents](/ingestion), and then use the [**Chat**](/chat) to explore your data.